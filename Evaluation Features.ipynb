{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "import traceback\n",
    "from collections import deque\n",
    "from __future__ import division\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '/scratch/users/rmth/tf_logs/c-shhs_cluster_revised-shallow_cv/'\n",
    "#path = '/scratch/users/rmth/tf_logs/c-shhs-multimodal_cluster_revised-shallow_cv/'\n",
    "#path = '/scratch/users/rmth/tf_logs/r-c-shhs_cluster_revised-shallow_cv/'\n",
    "path = '/scratch/users/rmth/tf_logs/r-c-shhs_cluster_revised-shallow-noise_cv/'\n",
    "fold_folders = [ e for e in os.listdir(path) if os.path.isdir(os.path.join(path, e)) ]\n",
    "print(fold_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p,:]\n",
    "\n",
    "def gen_data(exp, con, batch_size, max_length, n_features):\n",
    "    n = batch_size // 2\n",
    "    x = np.zeros(shape = (batch_size, max_length, n_features))\n",
    "    y = np.zeros(shape = (batch_size, 2))\n",
    "    shuffle(exp)\n",
    "    shuffle(con)\n",
    "    for i in range(n):\n",
    "        e = exp[i]\n",
    "        #print('e.shape1: {}'.format(e.shape[0]))\n",
    "        c = con[i]\n",
    "        #print('c.shape1: {}'.format(c.shape[0]))\n",
    "        x[i, 0:c.shape[0], 0:n_features] = c\n",
    "        y[i,0] = 1\n",
    "\n",
    "        x[i+n, 0:e.shape[0], 0:n_features] = e\n",
    "        y[i+n,1] = 1\n",
    "    return unison_shuffled_copies(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "n_dense = 16\n",
    "batch_size_train = 8\n",
    "regularization = 0.1\n",
    "dense_regularization = 0.1\n",
    "rnn_dropout_prob = 0.4\n",
    "lr = 1e-3 #was 1e-5\n",
    "#training_steps = 300 #1000\n",
    "display_step = 20\n",
    "buffer_length = 5\n",
    "final_dropout = 0.05\n",
    "#max_step = 10\n",
    "max_step = 3000 #3000\n",
    "tol = 1e-3\n",
    "patience = 5\n",
    "\n",
    "#Final: AUC=0.84693877551, acc.=0.785714268684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "n_dense = 1048\n",
    "batch_size_train = 8\n",
    "regularization = 0.1\n",
    "dense_regularization = 0.1\n",
    "rnn_dropout_prob = 0.4\n",
    "lr = 1e-4\n",
    "#training_steps = 300 #1000\n",
    "display_step = 20\n",
    "buffer_length = 3\n",
    "final_dropout = 0.02\n",
    "#max_step = 10\n",
    "max_step = 3000 #3000\n",
    "tol = 1e-2\n",
    "patience = 2\n",
    "\n",
    "#Final: AUC=0.510204081633, acc.=0.571428596973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "n_dense = 256\n",
    "batch_size_train = 8\n",
    "regularization = 0.1\n",
    "dense_regularization = 0.1\n",
    "rnn_dropout_prob = 0.4\n",
    "lr = 1e-4\n",
    "#training_steps = 300 #1000\n",
    "display_step = 20\n",
    "buffer_length = 3\n",
    "final_dropout = 0.05\n",
    "#max_step = 10\n",
    "max_step = 3000 #3000\n",
    "tol = 1e-2\n",
    "patience = 2\n",
    "\n",
    "#Test accuracy: 0.672 (CI: [0.6-0.746] +/- 0.109)\n",
    "#Test sens: 0.727 (CI: [0.653-0.813] +/- 0.116)\n",
    "#Test spec: 0.619 (CI: [0.475-0.759] +/- 0.196)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 1024\n",
    "n_dense = 256\n",
    "batch_size_train = 8\n",
    "regularization = 0.1\n",
    "dense_regularization = 0.1\n",
    "rnn_dropout_prob = 0.2\n",
    "lr = 1e-4\n",
    "display_step = 20\n",
    "buffer_length = 3\n",
    "final_dropout = 0.05\n",
    "max_step = 300 #WAS 300\n",
    "tol = 1e-2\n",
    "patience = 2\n",
    "\n",
    "#Test accuracy: 0.685 (CI: [0.629-0.742] +/- 0.0859)\n",
    "#Test sens: 0.674 (CI: [0.596-0.778] +/- 0.124)\n",
    "#Test spec: 0.695 (CI: [0.62-0.769] +/- 0.106)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "n_dense = 256\n",
    "batch_size_train = 8\n",
    "regularization = 0.1\n",
    "dense_regularization = 0.1\n",
    "rnn_dropout_prob = 0.1\n",
    "lr = 1e-3 \n",
    "display_step = 20\n",
    "buffer_length = 3\n",
    "final_dropout = 0.05\n",
    "max_step = 3000# 1000\n",
    "tol = 5#e1\n",
    "patience = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cv3 (1 of 9)\n"
     ]
    }
   ],
   "source": [
    "rocfigs, rocs = plt.subplots(ncols=3, nrows=3, figsize=(20,20))\n",
    "f1figs, f1s = plt.subplots(ncols=3, nrows=3, figsize=(20,20))\n",
    "senses_aggr = []\n",
    "speces_aggr = []\n",
    "accs_aggr = []\n",
    "cv_id_outcomes = {}\n",
    "\n",
    "senses_aggr_easy = []\n",
    "speces_aggr_easy = []\n",
    "accs_aggr_easy = []\n",
    "for fold_counter, fold in enumerate(fold_folders):\n",
    "    #if fold != 'cv6':\n",
    "    #    continue\n",
    "    try:\n",
    "        print('Training {} ({} of {})'.format(fold, fold_counter+1, len(fold_folders)))\n",
    "        fold_number = int(fold[2])\n",
    "        with open(path + fold + '/eval/' + 'features.pkl', 'rb') as f:\n",
    "            train_group, train_feat, val_group, val_feat, test_group, test_feat, matched_feat, matched_group = pickle.load(f)#, encoding='latin1')\n",
    "            \n",
    "        matched_ids = [e for e in matched_group.keys()]\n",
    "\n",
    "        train_label_exp = []\n",
    "        train_feat_exp = []\n",
    "        train_label_con = []\n",
    "        train_feat_con = []\n",
    "        for k, v in train_group.items():\n",
    "            if v:\n",
    "                train_label_exp.append(v)\n",
    "                train_feat_exp.append(train_feat[k])\n",
    "            else:\n",
    "                train_label_con.append(v)\n",
    "                train_feat_con.append(train_feat[k])\n",
    "\n",
    "        val_label = []\n",
    "        val_ft = []\n",
    "        for k,v in val_group.items():\n",
    "            if v:\n",
    "                val_label.append(v)\n",
    "                val_ft.append(val_feat[k])\n",
    "        n_exp_val = len(val_label)\n",
    "\n",
    "        test_label = []\n",
    "        test_ft = []\n",
    "        test_ids = []\n",
    "        for k,v in test_group.items():\n",
    "            if v:\n",
    "                test_label.append(v)\n",
    "                test_ft.append(test_feat[k])\n",
    "                test_ids.append(k)\n",
    "        n_exp_test = len(test_label)\n",
    "\n",
    "        matched_label_val = []\n",
    "        matched_feat_val = []\n",
    "        shuffle(matched_ids)\n",
    "        for k, v in matched_group.items():\n",
    "            if k in matched_ids[:n_exp_val]:\n",
    "                matched_label_val.append(v)\n",
    "                matched_feat_val.append(matched_feat[k])\n",
    "\n",
    "        matched_id_test = []\n",
    "        matched_label_test = []\n",
    "        matched_feat_test = []\n",
    "        for k, v in matched_group.items():\n",
    "            if k in matched_ids[n_exp_val:n_exp_val+n_exp_test]:\n",
    "                matched_label_test.append(v)\n",
    "                matched_feat_test.append(matched_feat[k])\n",
    "                matched_id_test.append(k)\n",
    "\n",
    "        matched_label_train = []\n",
    "        matched_feat_train = []\n",
    "        for k, v in matched_group.items():\n",
    "            if k in matched_ids[n_exp_val+n_exp_test:]:\n",
    "                matched_label_train.append(v)\n",
    "                matched_feat_train.append(matched_feat[k])\n",
    "        _,n_features = train_feat_exp[0].shape\n",
    "        max_length = np.max([len(e) for e in train_feat_exp + train_feat_con + val_ft + matched_feat_train + matched_feat_test])\n",
    "\n",
    "        x_test = np.zeros(shape=(n_exp_test*2, max_length, n_features))\n",
    "        y_test = np.zeros(shape=(n_exp_test*2, 2))\n",
    "        y_test[0:n_exp_test,0] = 1\n",
    "        y_test[n_exp_test:,1] = 1\n",
    "        x_test_ids = matched_id_test + test_ids        \n",
    "        for i,e in enumerate(matched_feat_test+test_ft):\n",
    "            x_test[i, 0:len(e), :] = e\n",
    "            x_test_ids\n",
    "            \n",
    "        x_val = np.zeros(shape=(n_exp_val*2, max_length, n_features))\n",
    "        y_val = np.zeros(shape=(n_exp_val*2, 2))\n",
    "        y_val[0:n_exp_val,0] = 1\n",
    "        y_val[n_exp_val:,1] = 1\n",
    "        for i,e in enumerate(matched_feat_val+val_ft):\n",
    "            x_val[i, 0:len(e), :] = e\n",
    "            \n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        x = tf.placeholder(shape=(None, max_length, n_features), dtype=tf.float32)\n",
    "        y = tf.placeholder(shape=(None, 2), dtype=tf.float32)\n",
    "        prob = tf.placeholder_with_default(1.0, shape=())\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        #lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n",
    "        #lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n",
    "\n",
    "        #lstm_fw_cell = tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(hidden_size)\n",
    "        #lstm_bw_cell = tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(hidden_size)\n",
    "        \n",
    "        #lstm_fw_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(hidden_size, dropout_keep_prob = prob)\n",
    "        #lstm_bw_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(hidden_size, dropout_keep_prob = prob)\n",
    "        \n",
    "        lstm_fw_cell = tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(hidden_size)\n",
    "        lstm_bw_cell = tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(hidden_size)\n",
    "\n",
    "        lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm_fw_cell, output_keep_prob=prob)\n",
    "        lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(lstm_bw_cell, output_keep_prob=prob)\n",
    "\n",
    "        #lstm_fw_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(hidden_size, dropout_keep_prob = prob)\n",
    "        #lstm_bw_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(hidden_size, dropout_keep_prob = prob)\n",
    "\n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, inputs=x, dtype=tf.float32)\n",
    "        \n",
    "        #print('outputs.shape: {}'.format(outputs[0].shape))\n",
    "        output_rnn = tf.concat(outputs, axis=2)\n",
    "        #print('output_rnn.shape: {}'.format(output_rnn.shape))\n",
    "\n",
    "        #print('states.shape: {}'.format(states[0].shape))\n",
    "        #states_flat = [tf.contrib.layers.flatten(e) for e in states]\n",
    "        #print('states_flat.shape: {}'.format(states_flat[0].shape))\n",
    "        #states_flat_concat = tf.concat(states_flat, axis=2)\n",
    "        #print('states_flat.shape: {}'.format(states_flat.shape))\n",
    "        \n",
    "        logit_input = tf.reshape(output_rnn, [batch_size, hidden_size*2*max_length])\n",
    "        dense = tf.layers.dense(inputs=logit_input,\n",
    "                                 units=n_dense,\n",
    "                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=dense_regularization),\n",
    "                                 activation=tf.nn.selu\n",
    "                                 )\n",
    "        do = tf.nn.dropout(dense,\n",
    "                        keep_prob=final_dropout)\n",
    "\n",
    "        logit = tf.layers.dense(inputs=do,\n",
    "                                 units=2)\n",
    "\n",
    "        tv = tf.trainable_variables()\n",
    "        regularization_cost = regularization * tf.reduce_sum([tf.nn.l2_loss(v) for v in tv])\n",
    "\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y)) + regularization_cost\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
    "        #optimizer = tf.train.RMSPropOptimizer(learning_rate=lr).minimize(cost)\n",
    "        pred = tf.argmax(logit,1)\n",
    "        correct_pred = tf.equal(pred, tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        softmax = tf.nn.softmax(logit)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            #for step in range(1, training_steps + 1):\n",
    "            step = 0\n",
    "            while True:\n",
    "                step += 1\n",
    "                #print(step)\n",
    "                #batch_x, batch_y = gen_data(val_ft, matched_feat_train, batch_size_train, max_length, n_features)\n",
    "                #batch_x, batch_y = gen_data(train_feat_exp, train_feat_con, batch_size_train, max_length, n_features)\n",
    "                batch_x, batch_y = gen_data(train_feat_exp, matched_feat_train, batch_size_train, max_length, n_features)\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, prob: rnn_dropout_prob})\n",
    "                if step % display_step == 0 or step == 1:\n",
    "                    acc, loss = sess.run([accuracy, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "                    acc_val, loss_val = sess.run([accuracy, cost], feed_dict = {x: x_val, y: y_val, prob: 1.0})\n",
    "                    if step == 1:\n",
    "                        smoothed_val_losses = deque(np.ones(buffer_length)*loss_val)\n",
    "                    else:\n",
    "                        mean1 = np.mean(smoothed_val_losses)\n",
    "                        smoothed_val_losses.popleft()\n",
    "                        smoothed_val_losses.append(loss_val)\n",
    "                        mean2 = np.mean(smoothed_val_losses)\n",
    "                        diff = mean1-mean2\n",
    "                        #print(diff)\n",
    "                        #print(smoothed_val_losses)\n",
    "                        #print(patience)\n",
    "                    if step > 2*buffer_length*display_step:\n",
    "                        if diff < tol:\n",
    "                            patience -= 1\n",
    "                        elif patience < 3:\n",
    "                            patience += 1\n",
    "                        if patience == 0 or step > max_step:\n",
    "                            break\n",
    "                    print('step: {}, train loss: {:.2f}, accuracy: {:.2f}, val loss: {:.2f}, val accuracy: {:.2f}'.format(step, loss, acc, loss_val, acc_val))\n",
    "            #for step in range(10):\n",
    "            #    batch_x, batch_y = gen_data(train_prob_exp, train_prob_con, batch_size_train, max_length)\n",
    "            #    sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, prob: 0.1})\n",
    "\n",
    "            x_test_easy, y_test_easy = gen_data(train_feat_exp, train_feat_con, batch_size_train, max_length, n_features)\n",
    "            acc_val, softmax_val = sess.run([accuracy, softmax], feed_dict = {x: x_val, y: y_val, prob: 1.0})\n",
    "            acc_test_easy, softmax_test_easy = sess.run([accuracy, softmax], feed_dict={x: x_test_easy, y: y_test_easy, prob: 1.0})\n",
    "            acc_test, softmax_test = sess.run([accuracy, softmax], feed_dict = {x: x_test, y: y_test, prob: 1.0})\n",
    "\n",
    "        fpr_val, tpr_val, thress_val = roc_curve(np.argmax(y_val,axis=1), softmax_val[:,1])\n",
    "        fpr_test_easy, tpr_test_easy, thress_test_easy = roc_curve(np.argmax(y_test_easy,axis=1), softmax_test_easy[:,1])\n",
    "        fpr_test, tpr_test, thress_test = roc_curve(np.argmax(y_test,axis=1), softmax_test[:,1])\n",
    "\n",
    "        \n",
    "        auc_val = auc(fpr_val, tpr_val)\n",
    "        auc_test = auc(fpr_test, tpr_test)\n",
    "        auc_test_easy = auc(fpr_test_easy, tpr_test_easy)\n",
    "\n",
    "        print('Final: AUC={}, acc.={}'.format(auc_test, acc_test))\n",
    "        \n",
    "        f1s_original = []\n",
    "        for t in thress_val:\n",
    "            f1s_original.append(f1_score(y_true=np.argmax(y_val,axis=1), y_pred=np.asarray(softmax_val[:,1] > t, dtype=np.float32)))\n",
    "\n",
    "        n_bootstraps = 300\n",
    "        thress_val = np.arange(0,1,0.05)\n",
    "        f1s_bootstraps = np.zeros((len(thress_val), n_bootstraps))\n",
    "        accs_bootstraps = np.zeros((len(thress_val), n_bootstraps))\n",
    "        true_val = np.argmax(y_val,axis=1)\n",
    "        est_val = softmax_val[:,1]\n",
    "        pairs = np.zeros((len(true_val), 2))\n",
    "        pairs[:,0] = true_val\n",
    "        pairs[:,1] = est_val\n",
    "        for j in range(n_bootstraps):\n",
    "            #if j % 100 == 0:\n",
    "            #    print(j/n_bootstraps*100)\n",
    "            pairs_boot = resample(pairs, n_samples = 20, replace=True)\n",
    "            true_val_boot = pairs_boot[:,0]\n",
    "            est_val_boot = pairs_boot[:,1]\n",
    "            for i, t in enumerate(thress_val):\n",
    "                tn, fp, fn, tp = metrics.confusion_matrix(true_val_boot, np.asarray(est_val_boot > t, dtype=np.float32)).ravel()\n",
    "                tpr = tp/(tp+fn)\n",
    "                fpr = fp/(tn+fp)\n",
    "                speci = 1-fpr\n",
    "                f1s_bootstraps[i, j] = 2 * (tpr * speci) / (tpr + speci + np.spacing(0))\n",
    "                #f1s_bootstraps[i, j] = f1_score(y_true=true_val_boot, y_pred=np.asarray(est_val_boot > t, dtype=np.float32))\n",
    "                accs_bootstraps[i, j] = accuracy_score(y_true=true_val_boot, y_pred=np.asarray(est_val_boot > t, dtype=np.float32))\n",
    "\n",
    "        f1s_m = np.median(f1s_bootstraps, axis=1)\n",
    "        accs = np.mean(accs_bootstraps, axis=1)\n",
    "\n",
    "        f1s_l = np.percentile(f1s_bootstraps, q=5, axis=1)\n",
    "        f1s_u = np.percentile(f1s_bootstraps, q=95, axis=1)\n",
    "        index = f1s_m/(f1s_u - f1s_l + np.spacing(0))\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        fig, ax = plt.subplots(ncols=3)\n",
    "        ax[0].plot(thress_val, f1s_m)\n",
    "        ax[0].plot(thress_val, f1s_l,'--')\n",
    "        ax[0].plot(thress_val, f1s_u,'--')\n",
    "        ax[1].plot(thress_val, index)\n",
    "        ax[2].plot(thress_val, f1s_original)\n",
    "        for i in range(3):\n",
    "            ax[i].set_xlim([0, 1])\n",
    "        ax[0].set_ylim([0,1])\n",
    "        ax[2].set_ylim([0,1])\n",
    "        '''\n",
    "\n",
    "        best_t_idx = np.nanargmax(index)\n",
    "        #best_t_idx = np.nanargmax(f1s_m)\n",
    "        best_t = thress_val[best_t_idx]\n",
    "\n",
    "        test_accuracy_easy = accuracy_score(y_true=np.argmax(y_test_easy, axis=1),\n",
    "                                       y_pred=np.asarray(softmax_test_easy[:, 1] > best_t, dtype=np.float32))\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(np.argmax(y_test_easy, axis=1), softmax_test_easy[:, 1] > best_t).ravel()\n",
    "        test_sensitivity_easy = tp / (tp + fn)\n",
    "        test_specificity_easy = 1 - (fp / (tn + fp))\n",
    "\n",
    "        accs_aggr_easy.append(test_accuracy_easy)\n",
    "        senses_aggr_easy.append(test_sensitivity_easy)\n",
    "        speces_aggr_easy.append(test_specificity_easy)\n",
    "\n",
    "        test_accuracy = accuracy_score(y_true=np.argmax(y_test,axis=1), y_pred=np.asarray(softmax_test[:,1] > best_t, dtype=np.float32))\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(np.argmax(y_test, axis=1), softmax_test[:, 1] > best_t).ravel()\n",
    "        test_sensitivity = tp / (tp + fn)\n",
    "        test_specificity = 1 - (fp / (tn+fp))\n",
    "\n",
    "        id_binary_outcome = {}\n",
    "        tmp_test_trues = np.argmax(y_test, axis=1)\n",
    "        tmp_test_estimates = softmax_test[:, 1] > best_t\n",
    "        outcomes = tmp_test_trues - 2*tmp_test_estimates\n",
    "        for i,e in enumerate(outcomes):\n",
    "            id_binary_outcome[x_test_ids[i]] = e\n",
    "        \n",
    "        cv_id_outcomes[fold] = id_binary_outcome\n",
    "        accs_aggr.append(test_accuracy)\n",
    "        senses_aggr.append(test_sensitivity)\n",
    "        speces_aggr.append(test_specificity)\n",
    "\n",
    "        lw = 2\n",
    "        a = (fold_number - 1) // 3\n",
    "        b = (fold_number - 1) % 3\n",
    "\n",
    "        rocs[a,b].plot(fpr_test_easy, tpr_test_easy, '--', color='gray',\n",
    "                   lw=lw,\n",
    "                   label='Unmatched, AUC: {:.2f}'.format(auc_test_easy))\n",
    "        rocs[a,b].plot(fpr_test, tpr_test, color='black',\n",
    "                   lw=lw,\n",
    "                   label='Matched, AUC: {:.2f}, acc.: {:.2f}, sens.: {:.2f}, spec.: {:.2f}'.format(auc_test,\n",
    "                                                                                                test_accuracy,\n",
    "                                                                                                test_sensitivity,\n",
    "                                                                                                test_specificity, ))\n",
    "        rocs[a,b].plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "        rocs[a,b].set_title('Cross-validation fold {}'.format(fold_number))\n",
    "        rocs[a,b].legend(loc=\"lower right\")\n",
    "        rocs[a,b].set_xlim([0.0, 1.0])\n",
    "        rocs[a,b].set_ylim([0.0, 1.0])\n",
    "        rocs[a,b].set_xticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        rocs[a,b].set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        if a == 2: \n",
    "            rocs[a,b].set_xlabel('False Positive Rate')\n",
    "        else:\n",
    "            rocs[a, b].set_xticklabels(['', '', '', '', '', ''])\n",
    "        if b == 0:\n",
    "            rocs[a,b].set_ylabel('True Positive Rate')\n",
    "        else:\n",
    "            rocs[a, b].set_yticklabels(['', '', '', '', '', ''])\n",
    "\n",
    "\n",
    "        f1s[a,b].plot(thress_val, f1s_m, color='black')\n",
    "        f1s[a, b].plot(thress_val[best_t_idx], f1s_m[best_t_idx], 'or')\n",
    "        f1s[a, b].set_title('F1-scores (validation, bootstrapped): {}'.format(fold_number))\n",
    "        f1s[a, b].set_ylim([0.0, 1.0])\n",
    "        rocs[a,b].set_xlim([0.0, 1.0])\n",
    "        f1s[a, b].set_xlabel('Threshold')\n",
    "        f1s[a, b].set_ylabel('f1-score')\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Something went wrong with: {}'.format(fold_number))\n",
    "        print(e)\n",
    "        print(traceback.format_exc())\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path + 'cv_id_outcomes.pkl', 'wb') as f:\n",
    "    pickle.dump([cv_id_outcomes], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_bootstraps = 1000\n",
    "test_accuracy_bootstraps_mean = np.ones(n_bootstraps)\n",
    "test_accuracy_bootstraps_std = np.ones(n_bootstraps)\n",
    "\n",
    "test_sens_bootstraps_mean = np.ones(n_bootstraps)\n",
    "test_sens_bootstraps_std = np.ones(n_bootstraps)\n",
    "\n",
    "test_spec_bootstraps_mean = np.ones(n_bootstraps)\n",
    "test_spec_bootstraps_std = np.ones(n_bootstraps)\n",
    "for j in range(n_bootstraps):\n",
    "    sample = resample(accs_aggr)\n",
    "    test_accuracy_bootstraps_mean[j] = np.mean(sample)\n",
    "    test_accuracy_bootstraps_std[j] = np.std(sample)\n",
    "\n",
    "    sample = resample(senses_aggr)\n",
    "    test_sens_bootstraps_mean[j] = np.mean(sample)\n",
    "    test_sens_bootstraps_std[j] = np.std(sample)\n",
    "\n",
    "    sample = resample(speces_aggr)\n",
    "    test_spec_bootstraps_mean[j] = np.mean(sample)\n",
    "    test_spec_bootstraps_std[j] = np.std(sample)\n",
    "\n",
    "acc_mean = np.mean(test_accuracy_bootstraps_mean)\n",
    "acc_std = np.mean(test_accuracy_bootstraps_std)\n",
    "acc_mean_l = np.percentile(test_accuracy_bootstraps_mean, 2.5)\n",
    "acc_mean_u = np.percentile(test_accuracy_bootstraps_mean, 97.5)\n",
    "print('Test accuracy: {:.3} (CI: [{:.3}-{:.3}] +/- {:.3})'.format(acc_mean, acc_mean_l, acc_mean_u, acc_std))\n",
    "\n",
    "sens_mean = np.mean(test_sens_bootstraps_mean)\n",
    "sens_std = np.mean(test_sens_bootstraps_std)\n",
    "sens_mean_l = np.percentile(test_sens_bootstraps_mean, 2.5)\n",
    "sens_mean_u = np.percentile(test_sens_bootstraps_mean, 97.5)\n",
    "print('Test sens: {:.3} (CI: [{:.3}-{:.3}] +/- {:.3})'.format(sens_mean, sens_mean_l, sens_mean_u, sens_std))\n",
    "\n",
    "spec_mean = np.mean(test_spec_bootstraps_mean)\n",
    "spec_std = np.mean(test_spec_bootstraps_std)\n",
    "spec_mean_l = np.percentile(test_spec_bootstraps_mean, 2.5)\n",
    "spec_mean_u = np.percentile(test_spec_bootstraps_mean, 97.5)\n",
    "print('Test spec: {:.3} (CI: [{:.3}-{:.3}] +/- {:.3})'.format(spec_mean, spec_mean_l, spec_mean_u, spec_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
