# stroke-deep-learning
The entire project was restructured to be more logically divided into folders, but only very little work was done to ensure that updated relative paths reflected this change (see the notebook on preprocessing for how it can be done). The training could not be tested due to lack of access to the SHHS data, so instead the library is presented as it was working in the spring of 2018 with no changes despite not working with the restructure. The module loading is shown in the preprocessing notebook, and the library was developed using tensorflow 1.8.0 and python 2.7 due to the availability of the newest tensorflow only for the old Python on Sherlock.

* /bash contains examples of the .sh scripts for running either preprocessing, training or inference using sherlock.
* /legacy are older files that were stored for easier access in case some old snippets were need, can be ignored.
* /notebooks contains some Jupyter notebooks that (at some point) could run the preprocessing, the training of both networks and do the evaluation, along with doing the interpretation tool. as of Mon Oct 15 only the pre-processing of files has been tested, and only on a single EDF, because the other SHHS files were removed from Sherlock.
  * Early stopping is a notebook that was used for determining if early stopping of training the CRNN was helpful, but it can be ignored.
  * Evaluation Features is a notebook that was used to train and evaluate the last RNN (i.e. the one that used the CRNN features as input). 
  * File-generation is a notebook that shows how the h5-files are created and preprocessed.
  * Inference is used to run the train CRNN models (at the moment the bottom part of it is a repeat of the STD notebook).
  * STD is used to do the Simple Taylor Decomposition Clustering (HiREC in the thesis). The first part of the notebook is the same as inference at the momemnt. 
  * Training is a notebook that shows how a CRNN is trained. 
* /resources contains configuration .yaml's for specifying various stuffs in a more easy to overview manner, similarly .list's are stored for hyperparameter tuning (meant to be used as arguments to some of the .sh scripts in /bash). Additionally, som IDs are stored for the cross-validation and matching of IDs (generated by shhs_group_generation.py in /src). 
* /src contains python classes and scripts for training and the likes.
  * channel_label_identifier.py is a script made by Hyatt Moore IV for determing aliases of channels in a cohort based on names found in EDFs
  * config.py is a class for loading and handling the configuration files (yaml-files). It relies on the hparam structure from tensorflow
  * datahandler.py is a class for getting batches of EEG data (X) and label (stroke, non-stroke, y). The datahdnler does this by means of the config.py, and the DataHandler enables cross validation upon initiation (see the DataHandler.setup_partitions class method). The way to use the cross-validation is to have the datahandler run for the first fold, which sets up the partitions, and then follow with training-calls for the other folds after the first fold partition setup has been made.
  * evaluation_features.py is a script used at some point for doing the last layer of RNN on features extracted from the CRNN (first layer of a CNN and RNN) by calling this script. Similarly, the evaluation_probabilities.py is the same, but instead of being based on features from the CRNN, it's based on only the probabilities assigned to each epoch. Both of these files can be ignored, seeing as this was later soely done using the notebook called Evaluation Features.ipynb.
  * gan.py is a model class for doing a conditional GAN (see the script for resources). The GAN is trained by setting up the object and then classing the object (see the call-function for the class). 
  * generate_h5_files.py can be ignored. The h5-file generation was in the end done by the notebook File-generation.ipynb.
  * inference.py is the script that allows for using a trained CRNN-model to do inference on the various EDFs in test, training and validation.  Similarly to how train.py works, it is based on using the python parser for instructions and usage with .sh-files stored in /bash
  * models.py is a class with the CRNN model. The model is defined by defining the various layers used as building blocks, all of which are then used to construct the network (see network private function), when the model is called. The buttom of the class specifies the input function which makes a generator of (X,y)-pairs used in training by calling the DataHandler. 
  * shhs_group_generation.py is a more clasic script (not an object/class), wherin files form SHHS is processed to determine which IDs go into test and training, and how a matched control set is made (to make sure the subjects are matched in e.g. age and BMI). The matched controls are made by a simple algorithm which minimizes a distance between the known stroke patients and a set of controls to be chosen. By iteratively choosing the "most like a stroke-patient"-control for each stroke-patient, we get a matched control group. The script does this, and also outputs some tables showing statistics for the matched controls versus stroke-patients, along with some baseline classification models (that performs poorly, as expected). 
  * train.py is a script that relies on getting parsed some arguments all descripted in the parser.add_arguments part of the script. The script is used for training the CRNN model defined in models.py, and relies on having a config object and a datahandler object to do the training. The script is mostly ran using the .sh files in /bash/, where e.g. hyperparamter screening is done by defining the experiements that you would like to run in a .list file which is then used as argument for the .sh-file that submits jobs. 
  * utils.py is a set of simple functions for processing signal or loading edfs, all which relates to the preprocessing parts of the using the model.
  * visualization.py is a script which simply loads an EDF and displays some plots to show how we segment the signal.
 
# Sherlock commands.

#### run interactive gpu:
srun -p gpu --gres gpu:1 --pty bash

#### tensorboard:
```
[zyzhang@sherlock-ln03 login_node ~]$ salloc -N 1 -n 1 --time=48:00:00 --gres=gpu:1 -p gpu,hns_gpu
[zyzhang@sherlock-ln03 login_node ~]$ squeue -u zyzhang

JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)

17689718 gpu bash zyzhang R 25:24 1 gpu-17-35

[zyzhang@sherlock-ln03 login_node ~]$ ssh -XY gpu-17-35

[zyzhang@gpu-17-35 /scratch/users/zyzhang/usertests/tensorflow/distributed]$ ml load tensorflow.1/1.3.0

[zyzhang@gpu-17-35 /scratch/users/zyzhang/usertests/tensorflow/distributed]$ tensorboard --logdir=/scratch/users/zyzhang/usertests/tensorflow/logdir

Starting TensorBoard 55 at http://gpu-17-35.local:6006
```
(Press CTRL+C to quit)

I am on a local linux machine and then I can display the tensorboard with the following:
```
[zyzhang@srn-exciton ~]$ ssh -L 8000:gpu-17-35:6006 zyzhang@sherlock.stanford.edu
```
after the above, you can display the tensorboard in a web browser with http://localhost:8000/

#### Jupyter notebooks on Sherlock

Using Jupyter notebooks on Sherlock
For those used to Jupyter notebooks, you can start them on Sherlock and access on your local machine fairly easy. The following assumes the user is using Python.
Login to Sherlock.
Request the specific resources for your needs. Eg: request a node with a GPU for 2 hours on either the mignot, gpu or owners partition, like so:
> srun -p gpu,mignot,owners --time=02:00:00 --gres gpu:1 --pty bash

Note the current node by running the following and commit to memory the value
> echo $SLURM_NODELIST

Activate your virtual environment (if you are using one) or just activate the Python binary
> ml load python/3.6.1

Change directory to your working directory (eg. where the notebooks are stored)
> cd ‘/path/to/directory’

Start the notebook and optionally assign a specific port number.
> jupyter notebook --no-browser --port=<remote_port#>

Now, on your local machine (eg. your laptop) first create a tunnel to the login nodes, and then create a tunnel from the login node to the specific node on which the notebook is running
> ssh -t -L <local_port#>:localhost:<login_port#> <sunetid>@login.sherlock.stanford.edu ssh -L <login_port#>:localhost:<remote_port#> <remote_node>

Now you can access the notebook in your browser by going to
https://localhost:<local_port#>
Hopefully, this will prompt you for a token (or password, if you have set this up on Sherlock).
