{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiation\n",
    "Import needed packages and define some functions for preprocessing.\n",
    "\n",
    "Make sure to have loaded all the needed modules before loading them here. This is done through e.g.:\n",
    "\n",
    "```\n",
    "module load py-h5py\n",
    "module load py-tensorflow/1.8.0_py27\n",
    "module load py-scipy\n",
    "module load py-scikit-learn\n",
    "module load py-scipystack\n",
    "module load python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently assuming resources are stored at:\n",
      "/home/users/rmth/stroke-deep-learning/resources\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../') #this allows us to import files form the ./stroke-deep-learning/src/-folder\n",
    "from src import utils\n",
    "from __future__ import division\n",
    "from os import listdir\n",
    "import utils\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from scipy.signal import butter, sosfilt, welch\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def determine_artefact_segments(x, fs, window_size, factors):   \n",
    "    f, t, Sxx = signal.spectrogram(x, fs, nperseg=window_size, noverlap=0)\n",
    "    ratio_high_frequency = np.sum(Sxx[f > 45,:],axis=0) / np.sum(Sxx[f < 20,:],axis=0)\n",
    "    power_high_frequency = np.sum(Sxx[f > 45,:],axis=0)\n",
    "    cond_ratio = ratio_high_frequency > np.min(ratio_high_frequency)*factors[0]\n",
    "    cond_power = power_high_frequency > np.min(power_high_frequency)*factors[1]\n",
    "    cond = cond_ratio+cond_power\n",
    "    artefact_idx = cond != 0\n",
    "    return t, artefact_idx\n",
    "\n",
    "def remove_artefacts(x, fs):\n",
    "    window_length=5\n",
    "    window_size = int(5*fs)\n",
    "    indices = []\n",
    "    tt = np.arange(0, x.shape[1]//fs,1/fs)\n",
    "    for i in range(x.shape[0]):\n",
    "        t,idx = determine_artefact_segments(np.squeeze(x[i,:]), fs, window_size,factors=[1e4,1e4])\n",
    "        f = interp1d(t, idx,'nearest', fill_value='extrapolate')\n",
    "        indices.append(f(tt).tolist())\n",
    "    i = np.sum(np.stack(indices),axis=0) == 0\n",
    "    out = x[:,i]\n",
    "    print('Before: {}, after: {}'.format(x.shape, out.shape))\n",
    "    return out\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    sos = butter(order, [low, high], btype='band', output='sos')\n",
    "    return sos\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
    "    sos = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = sosfilt(sos, data)\n",
    "    return y\n",
    "\n",
    "# Determine path to resources that we can define relatively based on their placement in the git repo\n",
    "notebook_folder = os.getcwd()\n",
    "resources_folder = os.path.abspath(os.path.join(notebook_folder, '../resources'))\n",
    "\n",
    "print('Currently assuming resources are stored at:')\n",
    "print(resources_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup paths and similar constants\n",
    "Setup all the paths and other settings. Some need to be hardcoded to the user running it, and so should be changed from their current setup. Thus block of code should specify:\n",
    "* which folder the EDFs are stored in\n",
    "* where the hypnograms (if relevant) are stored\n",
    "* which channels to load and how to orderthem/what to call them\n",
    "* how long of a duration the epoching for the output should be\n",
    "* where to output the processed data\n",
    "* which cohort we're creating files for\n",
    "* if we're doing simlated data\n",
    "* how we're doing the rescaling\n",
    "Note that the channel alias file needs to be manually changed to fit the need for which files are loaded and then stored in the designated output folder as a json. The json for channel aliases for the SHHS-Sherlock mode when a user runs this is therefore stored at:\n",
    "`/scratch/users/[user-name]/processed_shhs_data/signal_labels.json`\n",
    "and would look like:\n",
    "~~~~\n",
    "{\n",
    "    \"categories\": [\n",
    "        \"eeg1\",\n",
    "        \"eeg2\"\n",
    "    ],\n",
    "    \"edfFiles\": [\n",
    "        \"/home/rasmus/Desktop/shhs_subset/control/shhs1-203490.EDF\",\n",
    "        \"/home/rasmus/Desktop/shhs_subset/control/shhs1-205226.EDF\",\n",
    "        \"/home/rasmus/Desktop/shhs_subset/control/shhs1-205256.EDF\",\n",
    "        \"/home/rasmus/Desktop/shhs_subset/control/shhs1-203101.EDF\",\n",
    "        \"/home/rasmus/Desktop/shhs_subset/control/shhs1-203291.EDF\",\n",
    "        \"/home/rasmus/Desktop/shhs_subset/control/shhs1-205122.EDF\",\n",
    "        \"/home/rasmus/Desktop/shhs_subset/control/shhs1-205328.EDF\",\n",
    "        \"/home/rasmus/Desktop/shhs_subset/control/shhs1-204884.EDF\",\n",
    "        \"/home/rasmus/Desktop/shhs_subset/control/shhs1-203629.EDF\",\n",
    "        \"/home/rasmus/Desktop/shhs_subset/control/shhs1-203716.EDF\"\n",
    "    ],\n",
    "    \"eeg1\": [\n",
    "        \"EEG\"\n",
    "    ],\n",
    "    \"eeg2\": [\n",
    "        \"EEG 2\",\n",
    "\t\"EEG sec\",\n",
    "\t\"EEG(SEC)\",\n",
    "\t\"EEG(sec)\",\n",
    "\t\"EEG2\"\n",
    "    ],\n",
    "    \"pathname\": \"/home/rasmus/Desktop/shhs_subset/control\"\n",
    "} \n",
    "~~~~ \n",
    "\n",
    "This edFiles part is not important, as it is just a note on where the EDFs were stored when the tool for making the aliases was run. The tool used is stored in the repo as:\n",
    "\n",
    "\n",
    "`./stroke-deep-learning/src/channel_label_identifier.py`\n",
    "\n",
    "and was created by [Hyatt Moore IV](https://github.com/informaton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: consider implementing rejection of noise epochs\n",
    "debugging = False\n",
    "multimodal = False\n",
    "simulated_data = False\n",
    "revised_preprocessing = False\n",
    "rescale_mode = 'soft'\n",
    "#cohort = 'SHHS-Sherlock'\n",
    "cohort = 'SHHS-Sherlock-matched'\n",
    "#cohort = 'Simulated-Sherlock'\n",
    "\n",
    "if cohort == 'SSC':\n",
    "    edf_folder = '/home/rasmus/Desktop/SSC/raw/edf/'\n",
    "    hypnogram_folder = '/home/rasmus/Desktop/SSC/raw/hypnograms/'\n",
    "    epoch_duration = 5\n",
    "    channels_to_load = {'C3': 0, 'C4': 1, 'Fz': 2, 'ROC': 3, 'LOC': 4, 'Chin': 5}\n",
    "    output_folder = '/home/rasmus/Desktop/SSC/processed_data/'\n",
    "    IDs = listdir(edf_folder)\n",
    "    channel_alias = utils.read_channel_alias(edf_folder+'signal_labels.json')\n",
    "elif cohort == 'SHHS-Sherlock':\n",
    "    epoch_duration = 5*60\n",
    "    edf_folder = '/scratch/PI/mignot/nsrr/shhs/polysomnography/edfs/shhs1/'\n",
    "    hypnogram_folder = None\n",
    "    df = pd.read_csv('/home/users/rmth/stroke-deep-learning/IDs.csv')\n",
    "    IDs = np.asarray(df['IDs'])\n",
    "    group = np.asarray(df['group'])\n",
    "    if multimodal:\n",
    "        channels_to_load = {'eeg1': 0, 'eeg2': 1, 'ecg': 2, 'pulse': 3}\n",
    "        output_folder = '/scratch/users/rmth/processed_shhs_data_multimodal/'\n",
    "        channel_alias = utils.read_channel_alias(output_folder+'signal_labels_multimodal.json')\n",
    "    else:\n",
    "        channels_to_load = {'eeg1': 0, 'eeg2': 1}\n",
    "        if revised_preprocessing:\n",
    "            output_folder = '/scratch/users/rmth/processed_shhs_data_revised/'\n",
    "        else:\n",
    "            output_folder = '/scratch/users/rmth/processed_shhs_data/'\n",
    "        channel_alias = utils.read_channel_alias(output_folder+'signal_labels.json')\n",
    "elif cohort == 'SHHS-Sherlock-matched':\n",
    "    epoch_duration = 5*60\n",
    "    edf_folder = '/scratch/PI/mignot/nsrr/shhs/polysomnography/edfs/shhs1/'\n",
    "    hypnogram_folder = None\n",
    "    df = pd.read_csv(resources_folder + '/ids/matched_controls.csv')\n",
    "    IDs = np.asarray(df['conIDs'])\n",
    "    IDs = [int(e) for e in IDs]\n",
    "    group = np.asarray(np.zeros(len(IDs)))\n",
    "    if multimodal:\n",
    "        channels_to_load = {'eeg1': 0, 'eeg2': 1, 'ecg': 2, 'pulse': 3}\n",
    "        output_folder = '/scratch/users/rmth/processed_shhs_data_multimodal/'\n",
    "        channel_alias = utils.read_channel_alias(output_folder+'signal_labels_multimodal.json')\n",
    "    else:\n",
    "        channels_to_load = {'eeg1': 0, 'eeg2': 1}\n",
    "        if revised_preprocessing:\n",
    "            output_folder = '/scratch/users/rmth/processed_shhs_data_revised/matched_controls/'\n",
    "        else:\n",
    "            output_folder = '/scratch/users/rmth/processed_shhs_data/matched_controls/'\n",
    "        channel_alias = utils.read_channel_alias(output_folder+'signal_labels.json')\n",
    "elif cohort == 'SHHS':\n",
    "    epoch_duration = 5*60\n",
    "    edf_folder = '/home/rasmus/Desktop/shhs_subset/'\n",
    "    hypnogram_folder = None\n",
    "    control = listdir(edf_folder + 'control')\n",
    "    stroke = listdir(edf_folder + 'stroke')\n",
    "    if debugging:\n",
    "        control = control[0:1]\n",
    "        stroke = stroke[0:1]\n",
    "    group = np.concatenate((np.zeros(shape=len(control)),\n",
    "                            np.ones(shape= len(stroke))))\n",
    "    IDs = control+stroke\n",
    "    if multimodal:\n",
    "        channels_to_load = {'eeg1': 0, 'eeg2': 1, 'ecg': 2, 'pulse': 3}\n",
    "        output_folder = '/home/rasmus/Desktop/shhs_subset/processed_data_multimodal/'\n",
    "        channel_alias = utils.read_channel_alias(edf_folder+'signal_labels_multimodal.json')\n",
    "    elif simulated_data:\n",
    "        output_folder = '/home/rasmus/Desktop/shhs_subset/simulated_data/'\n",
    "    else:\n",
    "        channels_to_load = {'eeg1': 0, 'eeg2': 1}\n",
    "        output_folder = '/home/rasmus/Desktop/shhs_subset/processed_data/'\n",
    "        channel_alias = utils.read_channel_alias(edf_folder+'signal_labels.json')\n",
    "elif cohort == 'Simulated-Sherlock':\n",
    "    epoch_duration = 5*60\n",
    "    edf_folder = '/scratch/PI/mignot/nsrr/shhs/polysomnography/edfs/shhs1/'\n",
    "    hypnogram_folder = None\n",
    "    df = pd.read_csv('/home/users/rmth/stroke-deep-learning/IDs.csv')\n",
    "    IDs = np.asarray(df['IDs'])\n",
    "    group = np.asarray(df['group'])\n",
    "    channels_to_load = {'eeg1': 0, 'eeg2': 1}\n",
    "    output_folder = '/scratch/users/rmth/simulated_data/'\n",
    "    channel_alias = utils.read_channel_alias(output_folder+'signal_labels.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the following folders and outputs correspond to your expectations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Cohort:\n",
      "SHHS-Sherlock-matched\n",
      "## Data folder:\n",
      "/scratch/PI/mignot/nsrr/shhs/polysomnography/edfs/shhs1/\n",
      "## Hypnogram_folder:\n",
      "None\n",
      "## Output folder:\n",
      "/scratch/users/rmth/processed_shhs_data/matched_controls/\n",
      "## Channel alises:\n",
      "{u'EEG(sec)': u'eeg2', u'EEG(SEC)': u'eeg2', u'EEG sec': u'eeg2', u'EEG 2': u'eeg2', u'EEG': u'eeg1', u'EEG2': u'eeg2'}\n",
      "## IDs\n",
      "[200021, 200316, 200379, 200388, 200426, 200436, 200787, 200803, 200950, 200999, 201010, 201057, 201063, 201092, 201284, 201327, 201400, 201401, 201509, 201536, 201632, 201640, 201659, 201666, 201781, 201808, 201835, 201837, 201887, 201912, 201990, 201992, 202111, 202119, 202152, 202161, 202218, 202229, 202348, 202377, 202378, 202421, 202431, 202432, 202536, 202537, 202576, 202630, 202640, 202664, 202714, 202785, 202829, 202839, 202849, 202930, 202937, 202988, 203102, 203152, 203196, 203350, 203891, 203988, 204149, 204214, 204223, 204502, 204520, 204566, 205121, 205186, 205199, 205352, 205403, 205715]\n"
     ]
    }
   ],
   "source": [
    "print('## Cohort:')\n",
    "print(cohort)\n",
    "print('## Data folder:')\n",
    "print(edf_folder)\n",
    "print('## Hypnogram_folder:')\n",
    "print(hypnogram_folder)\n",
    "print('## Output folder:')\n",
    "print(output_folder)\n",
    "print('## Channel alises:')\n",
    "print(channel_alias)\n",
    "print('## IDs')\n",
    "print(IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if certain files have already recently been run. File IDs recently processed:\n",
      "['shhs1-201968.hpf5']\n"
     ]
    }
   ],
   "source": [
    "# If the processing crashes for some reason along with way,\n",
    "# it can be nice to not redo the entire thing for all files.\n",
    "# Here we check which we don't need to do.\n",
    "# However, if you want to do this, you need to add a line in \n",
    "# in the clock below to \"continue\" if the ID is in done_recently.\n",
    "import time\n",
    "path = output_folder\n",
    "done_recently = []\n",
    "now = time.time()\n",
    "for f in os.listdir(path):\n",
    "    if os.stat(os.path.join(path,f)).st_mtime > now - 0.5 * 86400:\n",
    "        done_recently.append(f)\n",
    "print('Checking if certain files have already recently been run. File IDs recently processed:')\n",
    "print(done_recently)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing\n",
    "No we're actually doing the processing. Note that the filtering is done below with a bandpass filter with cutoffs at .3 and 40 Hz. The code for using hypnograms are also only fit for used with the SSC data (Stanford Sleep Clinic).\n",
    "\n",
    "This implementation is a solution using try-catch, which is a poor way of handling errors in handling the EDFs, so make sure to check that what you output is in accordance with what you expect!\n",
    "\n",
    "> Someone (or automatic cleanup) has removed the SHHS EDFs from the mignot scratch, so only 4 remain. I've tested with one of them below, but to use the code again, the files needs to be reuploaded. -rmth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made it here\n",
      "Processing: 201968 (number 1 of 76).\n",
      "Actually processed /scratch/PI/mignot/nsrr/shhs/polysomnography/edfs/shhs1/shhs1-201968.edf\n",
      "All files processed (all processed will be mentioned, if IDs are listed, something went wrong).\n"
     ]
    }
   ],
   "source": [
    "debug = True # THIS SHOULD BE FALSE IF YOURE DOING SOMETHING FOR REAL\n",
    "\n",
    "if simulated_data:\n",
    "    flag = 0\n",
    "    n_records = 0\n",
    "    n_records_per_class = 20\n",
    "for counter, ID in enumerate(IDs):\n",
    "    if cohort == 'SHHS-Sherlock' or cohort == 'SHHS-Sherlock-matched' or cohort == 'Simulated-Sherlock':\n",
    "            if 'shhs1-' + str(ID) + \".hpf5\" in done_recently:\n",
    "                print('Done recently, skipping.')\n",
    "                continue\n",
    "    \n",
    "    if simulated_data:\n",
    "        if flag == 0:\n",
    "            if int(group[counter]) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                n_records += 1\n",
    "                if n_records == n_records_per_class:\n",
    "                    flag = 1\n",
    "                    n_records = 0\n",
    "        elif flag == 1:\n",
    "            if int(group[counter]) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                n_records += 1\n",
    "                if n_records == n_records_per_class:\n",
    "                    flag = 2\n",
    "                    n_records = 0\n",
    "        elif flag == 2:\n",
    "            break\n",
    "    \n",
    "    try:\n",
    "        #print('***WARNING: THIS IS DEBUGGING, BECAUSE EDFS HAVE BEEN REMOVED FROM SHERLOCK AND ONLY THIS AND A FEW OTHERS REMAIN FOR RMTH TO TEST')\n",
    "        #print(ID)\n",
    "        if debug:\n",
    "            ID = 201968\n",
    "            debug = False\n",
    "            \n",
    "        if ID != 201968:\n",
    "            continue\n",
    "        print('Made it here')\n",
    "        #if ID != 202345: # Different sampling frequency\n",
    "        #    continue\n",
    "        #if counter != 3:\n",
    "        #    continue\n",
    "            \n",
    "        print('Processing: ' + str(ID) + ' (number ' + str(counter+1) + ' of ' + str(len(IDs)) + ').')\n",
    "        if cohort == 'SSC':\n",
    "            filename = edf_folder + ID\n",
    "        elif cohort == 'SHHS-Sherlock' or cohort == 'SHHS-Sherlock-matched' or cohort == 'Simulated-Sherlock':\n",
    "            filename = edf_folder + 'shhs1-' + str(int(ID)) + '.edf'\n",
    "        elif cohort == 'SHHS':\n",
    "            if group[counter] == 1:\n",
    "                filename = edf_folder + 'stroke/' + ID\n",
    "            else:\n",
    "                filename = edf_folder + 'control/' + ID\n",
    "\n",
    "                \n",
    "        try:\n",
    "            data = utils.load_edf_file(filename, channels_to_load,\n",
    "                                       cohort = cohort,\n",
    "                                       channel_alias = channel_alias)\n",
    "        except Exception as e:\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            print(exc_type, fname, exc_tb.tb_lineno)\n",
    "            print(e)\n",
    "            print('    EDF error (loading failed).')\n",
    "            continue\n",
    "        if data == -1:\n",
    "            print('    Ignoring this subject due to different header setup.')\n",
    "            continue\n",
    "\n",
    "        if revised_preprocessing:\n",
    "            z = remove_artefacts(data['x'],data['fs'])\n",
    "            x = butter_bandpass_filter(z, .3, 40, data['fs'], order=32)\n",
    "        else:\n",
    "            x = butter_bandpass_filter(data['x'],0.3, 40.0, data['fs'], order=8)\n",
    "            \n",
    "        n = x.shape[1]\n",
    "        epoch_samples = epoch_duration * data['fs']\n",
    "        n_epochs = n // epoch_samples\n",
    "        if hypnogram_folder:\n",
    "            filename = hypnogram_folder + ID[:-4] + '.STA'\n",
    "            try:\n",
    "                hypnogram = utils.load_hypnogram_file(filename)\n",
    "                if epoch_duration != 30:\n",
    "                    hypnogram = np.repeat(hypnogram, repeats=30 // epoch_duration)\n",
    "                if n != hypnogram.shape[0]:\n",
    "                    hypnogram = hypnogram[:n_epochs]\n",
    "            except:\n",
    "                print('    Hypnogram error')\n",
    "                continue\n",
    "\n",
    "        epoched = np.zeros((int(len(channels_to_load)), int(n_epochs), int(epoch_samples)))\n",
    "        for i in range(len(channels_to_load)):\n",
    "            epoched[int(i), :, :] = np.asarray(list(zip(*[iter(x[int(i),:])] * int(epoch_samples))))\n",
    "            \n",
    "        if simulated_data:\n",
    "            noise = np.random.normal(0, .25, epoched.shape)\n",
    "            noise_complex = utils.add_known_complex(noise, data['fs'], group = int(group[counter]))\n",
    "            x = noise_complex \n",
    "        else:\n",
    "            x = utils.rescale(epoched, data['fs'], rescale_mode)\n",
    "\n",
    "        if cohort == 'SSC':\n",
    "            stages_to_use = [5,2]\n",
    "            stage_names = ['wake','n1','n2','n3','n4','rem','unknown','artefact']\n",
    "            for group, stage in enumerate(stages_to_use):\n",
    "                output_file_name = output_folder +  ID[:-4] + '_' + stage_names[stage] + \".hpf5\"\n",
    "                with h5py.File(output_file_name, \"w\") as f:\n",
    "                    dset = f.create_dataset(\"x\", data=x[:, hypnogram == stage, :], chunks=True)\n",
    "                    f['fs'] = data['fs']\n",
    "                    f[\"group\"] = group\n",
    "        elif cohort == 'SHHS-Sherlock' or cohort == 'SHHS-Sherlock-matched' or cohort == 'Simulated-Sherlock':\n",
    "            output_file_name = output_folder + 'shhs1-' + str(ID) + \".hpf5\"\n",
    "            with h5py.File(output_file_name, \"w\") as f:\n",
    "                dset = f.create_dataset('x', data=x, chunks=True)\n",
    "                f['fs'] = data['fs']\n",
    "                f['group'] = int(group[counter])\n",
    "        elif cohort == 'SHHS':\n",
    "            output_file_name = output_folder +  ID[:-4] + \".hpf5\"\n",
    "            with h5py.File(output_file_name, \"w\") as f:\n",
    "                dset = f.create_dataset(\"x\", data=x, chunks=True)\n",
    "                f['fs'] = data['fs']\n",
    "                f[\"group\"] = group[counter]\n",
    "        print('Actually processed {}'.format(filename))\n",
    "    except Exception as e:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "        print(e)\n",
    "        print('Error happened while processing: {}'.format(str(filename)))\n",
    "\n",
    "print(\"All files processed (all processed will be mentioned, if IDs are listed, something went wrong).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
